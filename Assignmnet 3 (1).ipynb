{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<autogen_ext.models.openai._openai_client.OpenAIChatCompletionClient object at 0x1551d9fd0> <autogen_ext.models.openai._openai_client.OpenAIChatCompletionClient object at 0x1553747d0> <autogen_ext.models.openai._openai_client.OpenAIChatCompletionClient object at 0x1553dacf0>\n",
            "Extracted 24 chunks from paper1.pdf\n",
            "Extracted 17 chunks from paper2.pdf\n",
            "Extracted 15 chunks from paper3.pdf\n",
            "Extracted 12 chunks from paper4.pdf\n",
            "Extracted 17 chunks from paper5.pdf\n",
            "Extracted 21 chunks from paper6.pdf\n",
            "Processing papers 1-2 with 41 chunks...\n",
            "Word count: 421\n",
            "Processing papers 3-4 with 27 chunks...\n",
            "Word count: 478\n",
            "Processing papers 5-6 with 38 chunks...\n",
            "Word count: 1367\n",
            "Compiling into 500-word summary...\n",
            "Word count: 524\n",
            "\n",
            "Final 1000-Word Summary:\n",
            "Here is a unified summary of the six papers on multi-agent systems powered by large language models (LLMs) in exactly 1000 words:\n",
            "\n",
            "Multi-agent systems powered by large language models (LLMs) have been explored in various domains, including problem-solving, decision-making, and communication. This approach mimics human group work, where agents engage in planning, discussions, and decision-making, leveraging the communicative capabilities of LLMs. The agents can respond to textual inputs, generate text for communication, and exploit knowledge across various domains.\n",
            "\n",
            "The architecture of LLM-based multi-agent (LLM-MA) systems involves multiple autonomous agents that collaborate to achieve a common goal. The agents interact with the task environment, exchange messages, and learn from each other to improve their capabilities. The system's performance is evaluated based on its alignment with the operational environment and collective objectives.\n",
            "\n",
            "In LLM-MA systems, agents are defined by their traits, actions, and skills, which are tailored to meet specific goals. The agents can assume distinct roles, each with comprehensive descriptions encompassing characteristics, capabilities, behaviors, and constraints. The communication structure can be either decentralized, where agents directly communicate with each other, or centralized, where a central agent coordinates the system's communication.\n",
            "\n",
            "The agents can leverage a memory module to adjust their behavior, storing information from previous interactions and feedback to retrieve relevant memories when performing actions. They can also engage in self-evolution, relying on historical records to decide subsequent actions.\n",
            "\n",
            "LLM-MA systems have applications in various domains, including software development, gaming, economic simulation, and policy making. In software development, LLM-MA systems can emulate distinct roles, such as product managers, programmers, and testers, to address complex challenges. In gaming, LLM-MA systems can create simulated environments, allowing agents to assume various roles within games. In economic simulation, LLM-MA systems can model real-world scenarios, such as labor markets and information marketplaces.\n",
            "\n",
            "However, LLM-MA systems also face challenges, including the risk of misinformation propagation, where inaccurate information from one agent can spread to others in the network. Evaluating LLM-MA systems and benchmarking their performance is also a significant challenge.\n",
            "\n",
            "Several frameworks and benchmarks have been proposed to facilitate the development and evaluation of LLM-MA systems. For example, the ChatEval framework uses a multi-agent debate process to evaluate text quality, aligning better with human preferences compared to single-agent-based approaches. The framework has been shown to enhance the performance of evaluation metrics and provide more reliable results.\n",
            "\n",
            "Task allocation and communication are crucial aspects of LLM-MA systems. Effective task allocation ensures that agents optimize their performance, while communication enables coordination and collaboration. Challenges in task allocation include scalability, where the complexity of communication and coordination grows with the number of agents, and task heterogeneity, where agents have diverse skills and expertise.\n",
            "\n",
            "LLM-MA systems have the potential to revolutionize various domains by enabling collaborative problem-solving, decision-making, and communication. However, addressing the challenges and limitations of these systems is crucial to ensure their effective deployment.\n",
            "\n",
            "In conclusion, LLM-MA systems have the potential to transform various fields by enabling collaborative problem-solving, decision-making, and communication. The development of frameworks, benchmarks, and evaluation metrics is essential for the advancement of these systems. Addressing the challenges and limitations of LLM-MA systems is crucial to ensure their effective deployment.\n",
            "\n",
            "Score (BLEU or Overlap): 0.3740\n"
          ]
        }
      ],
      "source": [
        "# literature_review.py\n",
        "import os\n",
        "import fitz\n",
        "import asyncio\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen import UserProxyAgent\n",
        "from autogen_agentchat.conditions import MaxMessageTermination\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "import logging\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from collections import Counter\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = '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'\n",
        "\n",
        "# LLM Clients\n",
        "model_client_llama = OpenAIChatCompletionClient(\n",
        "    model=\"llama3-70b-8192\", base_url=\"https://api.groq.com/openai/v1\", api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_info={\"vision\": False, \"function_calling\": True, \"json_output\": False, \"family\": \"llama\"})\n",
        "model_client_deepseek = OpenAIChatCompletionClient(\n",
        "    model=\"deepseek-r1-distill-llama-70b\", base_url=\"https://api.groq.com/openai/v1\", api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_info={\"vision\": False, \"function_calling\": True, \"json_output\": False, \"family\": \"deepseek\"})\n",
        "model_client_mistralai = OpenAIChatCompletionClient(\n",
        "    model=\"mistral-saba-24b\", base_url=\"https://api.groq.com/openai/v1\", api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model_info={\"vision\": False, \"function_calling\": True, \"json_output\": False, \"family\": \"mistral\"})\n",
        "print(model_client_llama, model_client_deepseek, model_client_mistralai)\n",
        "\n",
        "# Tools\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\".join(page.get_text() for page in doc)\n",
        "    doc.close()\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, max_tokens=1000):\n",
        "    words = text.split()\n",
        "    chunks, current_chunk, current_tokens = [], [], 0\n",
        "    for word in words:\n",
        "        word_tokens = len(word) // 4 + 1\n",
        "        if current_tokens + word_tokens > max_tokens:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk, current_tokens = [word], word_tokens\n",
        "        else:\n",
        "            current_chunk.append(word)\n",
        "            current_tokens += word_tokens\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "# Agents\n",
        "summarizer1 = AssistantAgent(\n",
        "    name=\"Summarizer1\", model_client=model_client_llama,\n",
        "    system_message=(\n",
        "        \"Summarize the provided chunks from papers 1 and 2 into exactly 1000 words, focusing on multi-agent systems powered by large language models (LLMs). \"\n",
        "        \"Cover the papers’ main contributions to multi-agent LLM collaboration, including key ideas (e.g., how agents work together, communicate, or solve problems), \"\n",
        "        \"notable challenges, and any mentioned applications or frameworks. \"\n",
        "        \"Keep the summary clear, concise, and relevant, using a formal tone suitable for an academic review. \"\n",
        "        \"Base your summary only on the chunks provided, aiming for exactly 1000 words. End with 'TERMINATE'.\"\n",
        "    ),\n",
        "    description=\"Generates a 1000-word summary from papers 1-2 on multi-agent LLM systems.\"\n",
        ")\n",
        "\n",
        "summarizer2 = AssistantAgent(\n",
        "    name=\"Summarizer2\", model_client=model_client_mistralai,\n",
        "    system_message=(\n",
        "        \"Summarize the provided chunks from papers 3 and 4 into exactly 1000 words, focusing on multi-agent systems powered by large language models (LLMs). \"\n",
        "        \"Highlight the papers’ key contributions to multi-agent LLM collaboration, including main ideas (e.g., agent coordination, decision-making), \"\n",
        "        \"significant challenges, and any applications or frameworks discussed. \"\n",
        "        \"Use a clear, concise, formal tone appropriate for an academic summary. \"\n",
        "        \"Stay within the chunks’ content, targeting exactly 1000 words. End with 'TERMINATE'.\"\n",
        "    ),\n",
        "    description=\"Generates a 1000-word summary from papers 3-4 on multi-agent LLM systems.\"\n",
        ")\n",
        "\n",
        "summarizer3 = AssistantAgent(\n",
        "    name=\"Summarizer3\", model_client=model_client_deepseek,\n",
        "    system_message=(\n",
        "        \"Summarize the provided chunks from papers 5 and 6 into exactly 1000 words, focusing on multi-agent systems powered by large language models (LLMs). \"\n",
        "        \"Address the papers’ primary contributions to multi-agent LLM collaboration, such as key concepts (e.g., task allocation, communication), \"\n",
        "        \"important challenges, and any frameworks or applications presented. \"\n",
        "        \"Write in a clear, formal tone for an academic audience, keeping it concise and chunk-based. \"\n",
        "        \"Ensure exactly 1000 words and end with 'TERMINATE'.\"\n",
        "    ),\n",
        "    description=\"Generates a 1000-word summary from papers 5-6 on multi-agent LLM systems.\"\n",
        ")\n",
        "\n",
        "compiler = AssistantAgent(\n",
        "    name=\"Compiler\", model_client=model_client_llama,\n",
        "    system_message=(\n",
        "        \"Combine the three 1000-word summaries into a single 1000-word summary on multi-agent systems powered by large language models (LLMs). \"\n",
        "        \"Create a cohesive overview by: (1) introducing the collective scope of the six papers, \"\n",
        "        \"(2) blending their key contributions (e.g., how LLMs enable agent collaboration), \"\n",
        "        \"(3) summarizing shared or differing challenges, and (4) highlighting notable applications or frameworks. \"\n",
        "        \"Identify connections (e.g., similar ideas, complementary solutions) to form a unified narrative. \"\n",
        "        \"Use a clear, formal tone, avoid repetition, and ensure exactly 1000 words. End with 'TERMINATE'.\"\n",
        "    ),\n",
        "    description=\"Compiles three 1000-word summaries into a cohesive 1000-word summary on multi-agent LLM systems.\"\n",
        ")\n",
        "\n",
        "human_proxy = UserProxyAgent(\n",
        "    name=\"HumanProxy\",\n",
        "    human_input_mode=\"ALWAYS\",\n",
        "    max_consecutive_auto_reply=0,\n",
        "    code_execution_config={\"use_docker\": False},\n",
        "    description=\"Human overseer for review and adjustments.\"\n",
        ")\n",
        "\n",
        "# Teams\n",
        "team1 = RoundRobinGroupChat([summarizer1, human_proxy], termination_condition=MaxMessageTermination(max_messages=3))\n",
        "team2 = RoundRobinGroupChat([summarizer2, human_proxy], termination_condition=MaxMessageTermination(max_messages=3))\n",
        "team3 = RoundRobinGroupChat([summarizer3, human_proxy], termination_condition=MaxMessageTermination(max_messages=3))\n",
        "team_compiler = RoundRobinGroupChat([compiler, human_proxy], termination_condition=MaxMessageTermination(max_messages=3))\n",
        "\n",
        "async def process_paper_chunks(team, chunks, paper_range, summarizer_name, paper_start, paper_end):\n",
        "    paper_chunks = chunks[paper_range[0]:paper_range[1]]\n",
        "    task = f\"Process papers {paper_start}-{paper_end}:\\n\\n\" + \"\\n\\n\".join(\n",
        "        f\"Chunk {i+1}: {chunk[:500]}...\" for i, chunk in enumerate(paper_chunks))\n",
        "    print(f\"Processing papers {paper_start}-{paper_end} with {len(paper_chunks)} chunks...\")\n",
        "    logging.info(f\"Started processing papers {paper_start}-{paper_end}\")\n",
        "    async for msg in team.run_stream(task=task):\n",
        "        if hasattr(msg, 'source') and msg.source == summarizer_name:\n",
        "            content = msg.content.strip()\n",
        "            if content and \"TERMINATE\" in content:\n",
        "                word_count = count_words(content.replace('TERMINATE', ''))\n",
        "                print(f\"Word count: {word_count}\")\n",
        "                logging.info(f\"Summary for papers {paper_start}-{paper_end}: {content}\")\n",
        "                return content.replace('TERMINATE', '').strip()\n",
        "    return None\n",
        "\n",
        "# Load and chunk papers\n",
        "all_chunks = []\n",
        "paper_boundaries = [0]\n",
        "for i in range(1, 7):\n",
        "    try:\n",
        "        text = extract_text_from_pdf(f\"paper{i}.pdf\")\n",
        "        paper_chunks = chunk_text(text)\n",
        "        all_chunks.extend(paper_chunks)\n",
        "        paper_boundaries.append(len(all_chunks))\n",
        "        print(f\"Extracted {len(paper_chunks)} chunks from paper{i}.pdf\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing paper{i}.pdf: {e}\")\n",
        "\n",
        "paper_ranges = [(paper_boundaries[0], paper_boundaries[2]), (paper_boundaries[2], paper_boundaries[4]),\n",
        "                (paper_boundaries[4], paper_boundaries[6])]\n",
        "\n",
        "async def main_step1():\n",
        "    summary1 = await process_paper_chunks(team1, all_chunks, paper_ranges[0], \"Summarizer1\", 1, 2)\n",
        "    summary2 = await process_paper_chunks(team2, all_chunks, paper_ranges[1], \"Summarizer2\", 3, 4)\n",
        "    summary3 = await process_paper_chunks(team3, all_chunks, paper_ranges[2], \"Summarizer3\", 5, 6)\n",
        "    return [summary1, summary2, summary3]\n",
        "\n",
        "async def compile_summaries(team, summaries):\n",
        "    task = f\"Compile these summaries:\\n\\n\" + \"\\n\\n\".join(f\"Summary {i+1}: {s}\" for i, s in enumerate(summaries))\n",
        "    print(\"Compiling into 500-word summary...\")\n",
        "    logging.info(\"Started compiling 1000-word summary\")\n",
        "    async for msg in team.run_stream(task=task):\n",
        "        if hasattr(msg, 'source') and msg.source == 'Compiler':\n",
        "            content = msg.content.strip()\n",
        "            if content and \"TERMINATE\" in content:\n",
        "                word_count = count_words(content.replace('TERMINATE', ''))\n",
        "                print(f\"Word count: {word_count}\")\n",
        "                logging.info(f\"Final summary: {content}\")\n",
        "                return content.replace('TERMINATE', '').strip()\n",
        "    return None\n",
        "\n",
        "def evaluate_output(system_summary, manual_summary):\n",
        "    try:\n",
        "        system_words = system_summary.split()\n",
        "        manual_words = manual_summary.split()\n",
        "        bleu_score = sentence_bleu([manual_words], system_words)\n",
        "    except TypeError:\n",
        "        # Fallback: Simple overlap percentage if BLEU fails\n",
        "        system_set = set(system_summary.split())\n",
        "        manual_set = set(manual_summary.split())\n",
        "        overlap = len(system_set & manual_set) / len(manual_set) if manual_set else 0\n",
        "        bleu_score = overlap  # Using overlap as a proxy\n",
        "    return bleu_score\n",
        "async def main():\n",
        "    logging.info(\"Starting system test with 6 papers\")\n",
        "    summaries = await main_step1()\n",
        "    final_summary = await compile_summaries(team_compiler, summaries)\n",
        "    print(\"\\nFinal 1000-Word Summary:\")\n",
        "    print(final_summary)\n",
        "    \n",
        "    # Evaluation\n",
        "    manual_review = \"\"\"\n",
        "            This summary integrates insights from six papers on multi-agent systems powered by large language models (LLMs). These systems are designed to enhance communication, planning, and decision-making among agents, leveraging LLMs to generate and respond to textual inputs. Agents possess distinct traits, actions, and skills, tailored to specific goals, and assume roles with detailed descriptions of capabilities, behaviors, and constraints. Communication occurs through decentralized, centralized, or shared message pool structures.\n",
        "            The papers emphasize agent profiling, where LLMs define how agents behave, and capability acquisition, where agents learn from interactions to adjust their actions. Applications include software development, embodied AI, gaming, policy-making, and psychology, showcasing their versatility. Collaborative environments improve efficiency through specialized roles, supported by plugins for task-specific functions like data storage or external system interaction. An oracle agent, operating statelessly, refines responses with feedback.\n",
        "            Challenges include evaluating LLM-based multi-agent (LLM-MA) systems and benchmarking performance, as current frameworks struggle to capture emergent behaviors. Risks like misinformation and hallucinations require detection mechanisms, while security and privacy demand input validation and encryption. Adaptive structures are needed to dynamically add or remove agents for complex tasks.\n",
        "            Techniques such as debate processes align agents with human objectives, while inception prompting guides task fulfillment. The papers explore LLM integration into multi-agent reinforcement learning (MARL), particularly within Decentralized Partially Observable Markov Decision Process (Dec-POMDP) frameworks. Communication is critical in dynamic environments, enabling agents to adapt strategies, align with peers, and optimize cooperative or competitive outcomes. Frameworks like Retroformer and CoELA enhance decision-making, and AutoAggents drafts collaboration plans, defining agent outputs.\n",
        "            Research directions include developing evaluation metrics for task performance and communication efficiency, exploring competitive MARL, and integrating human roles into language-conditioned MARL. Security concerns, such as protecting against harmful inputs, remain vital. The papers propose frameworks to optimize LLM-based MARL, emphasizing communication and adaptability across applications.\n",
        "            In conclusion, multi-agent systems powered by LLMs offer significant potential for complex tasks through collaboration. Despite challenges like evaluation, security, and scalability, ongoing research focuses on refining communication protocols, mitigating risks, and expanding collaborative and competitive applications.\n",
        "        \"\"\"\n",
        "    bleu_score = evaluate_output(final_summary, manual_review)\n",
        "    print(f\"\\nScore (BLEU or Overlap): {bleu_score:.4f}\")\n",
        "    logging.info(f\"Evaluation completed. Score: {bleu_score:.4f}\")\n",
        "\n",
        "# Run in Jupyter\n",
        "await main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 24 chunks from paper1.pdf\n",
            "Extracted 17 chunks from paper2.pdf\n",
            "Extracted 15 chunks from paper3.pdf\n",
            "Extracted 12 chunks from paper4.pdf\n",
            "Extracted 17 chunks from paper5.pdf\n",
            "Extracted 21 chunks from paper6.pdf\n",
            "Processing paper 1 with 24 chunks...\n",
            "Word count: 442\n",
            "Processing paper 2 with 17 chunks...\n",
            "Word count: 394\n",
            "Processing paper 3 with 15 chunks...\n",
            "Word count: 491\n",
            "Processing paper 4 with 12 chunks...\n",
            "Word count: 391\n",
            "Processing paper 5 with 17 chunks...\n",
            "Word count: 352\n",
            "Processing paper 6 with 21 chunks...\n",
            "Word count: 393\n",
            "Compiling into 1000-word summary...\n",
            "Word count: 1382\n",
            "\n",
            "Final 1000-Word Summary:\n",
            "<think>\n",
            "Okay, I need to help the user synthesize six 500-word summaries into a 1000-word summary on multi-agent systems powered by LLMs. The structure should include an introduction, key concepts, challenges, and applications. Let me start by understanding each summary.\n",
            "\n",
            "Summary 1 introduces LLM-MA systems, their architecture, and applications. It also mentions challenges like evaluation and misinformation. Summary 2 discusses ChatEval, a multi-agent framework for text evaluation, highlighting role-based interaction and improved performance. Summary 3 presents a collaborative framework where agents specialize, use plugins, and have an LLM as a designer. It also notes ethical challenges.\n",
            "\n",
            "Summary 4 focuses on challenges like task allocation, reasoning, and memory management. It applies these to areas like blockchain and fraud detection. Summary 5 covers LLM-based MARL, discussing techniques like roles, communication, and learning, while noting challenges in evaluation and security. Summary 6 introduces AgentCoord, a visual framework for coordination, addressing cognitive overhead and flexibility.\n",
            "\n",
            "Now, I need to integrate these into a cohesive 1000-word summary. The introduction should frame the collective scope. I'll mention the transformative potential and the papers' focus on architecture, collaboration, and applications.\n",
            "\n",
            "For key concepts, I'll cover communication methods (decentralized, centralized), learning techniques (reinforcement, iterative debates), and memory management. I'll also include frameworks like ChatEval, CoELA, and AgentCoord, highlighting how each contributes uniquely.\n",
            "\n",
            "Challenges should be consolidated, noting common issues like evaluation metrics, task allocation, and ethical concerns. Applications will link each paper's contributions, showing how they apply across domains like software, law, gaming, etc.\n",
            "\n",
            "I need to ensure a clear narrative flow, connecting each paper's ideas. For example, how Summary 4's challenges in task allocation tie into Summary 3's specialized agents. Also, how AgentCoord from Summary 6 helps in strategy design mentioned in Summary 1.\n",
            "\n",
            "I must avoid repetition and maintain a formal tone. Each section should be concise, leading up to the conclusion that emphasizes the potential and need for addressing challenges. Finally, end with '' as specified.\n",
            "\n",
            "Let me structure the sections: Introduction, Key Concepts, Challenges, Applications, and Conclusion. Each part should flow logically, referencing each summary without duplicating content. I'll ensure exactly 1000 words, so I'll be concise in each section.\n",
            "\n",
            "I think that's a solid plan. Now, I'll draft each part, making sure to weave in each summary's contributions seamlessly.\n",
            "</think>\n",
            "\n",
            "**Introduction**\n",
            "\n",
            "The integration of large language models (LLMs) into multi-agent systems (MAS) has opened new avenues for advancing artificial intelligence, enabling agents to collaborate, communicate, and solve complex tasks more effectively. This synthesis of six papers explores the collective scope of LLM-powered MAS, focusing on key concepts, techniques, challenges, and applications. These papers collectively highlight the transformative potential of LLMs in enhancing the autonomy, adaptability, and problem-solving capabilities of MAS, while also addressing the limitations and open challenges in this rapidly evolving field.\n",
            "\n",
            "**Key Concepts and Techniques**\n",
            "\n",
            "The papers collectively identify several key concepts and techniques that underpin the development and operation of LLM-powered MAS. These include communication structures, learning mechanisms, and memory management systems, which are critical for enabling effective collaboration among agents.\n",
            "\n",
            "1. **Communication and Collaboration**: Communication is a cornerstone of MAS, and LLMs provide a natural medium for agents to interact. Papers 1 and 2 explore decentralized, centralized, and shared message pool communication structures, while Paper 6 introduces a visual framework (AgentCoord) to facilitate coordination strategies. The use of natural language enables agents to engage in nuanced interactions, such as debates (Paper 2) or role-based discussions (Paper 3), which enhance problem-solving and decision-making.\n",
            "\n",
            "2. **Learning and Adaptation**: LLMs enable agents to learn from interactions and adapt to changing environments. Paper 1 highlights the role of memory modules in storing and retrieving interactions to inform future actions. Paper 5 extends this concept to reinforcement learning, proposing frameworks like CoELA (Cooperative Embodied Language Agent) and Retroformer to integrate LLMs with RL for embodied applications. These frameworks demonstrate how LLMs can generate feedback and guide agents in complex tasks.\n",
            "\n",
            "3. **Role Specialization and Task Allocation**: The papers emphasize the importance of assigning roles to agents to enhance efficiency and effectiveness. Paper 3 proposes a collaborative framework where agents specialize in specific functions, while Paper 4 highlights the need for optimizing task allocation and fostering robust reasoning through iterative interactions. Paper 6 further supports this idea by providing a structured representation for coordination strategies, enabling users to explore and assign roles effectively.\n",
            "\n",
            "4. **Memory and Context Management**: Managing memory and context is critical for maintaining coherence in MAS. Paper 1 introduces memory modules that store and retrieve interactions, while Paper 4 classifies memories into short-term, long-term, consensus, and external storage, highlighting the complexity of memory management in multi-agent systems.\n",
            "\n",
            "**Challenges and Limitations**\n",
            "\n",
            "While LLM-powered MAS offer significant advantages, they also face several challenges and limitations. These include:\n",
            "\n",
            "1. **Evaluation and Benchmarking**: Papers 1, 2, and 5 highlight the difficulty of evaluating and comparing the performance of LLM-based MAS. The lack of standardized metrics for assessing communication quality, task performance, and system coherence poses a significant challenge.\n",
            "\n",
            "2. **Task Allocation and Reasoning**: Paper 4 emphasizes the complexity of optimizing task allocation and fostering robust reasoning in MAS. Ensuring that agents work together cohesively while addressing diverse and dynamic tasks remains a critical challenge.\n",
            "\n",
            "3. **Memory and Context Management**: The sophisticated structure of memory in MAS, as discussed in Paper 4, introduces challenges in maintaining consistency and alignment with system objectives. Managing layered context information while ensuring coherence is a significant technical hurdle.\n",
            "\n",
            "4. **Ethical and Security Concerns**: Papers 3 and 5 raise ethical considerations, such as ensuring fairness and preventing bias in agent interactions. Additionally, the security of LLMs against adversarial attacks is a growing concern, particularly in applications like fraud detection and blockchain.\n",
            "\n",
            "5. **Cognitive Overhead and Flexibility**: Paper 6 identifies cognitive overhead as a major limitation in designing coordination strategies for MAS. Users often struggle to explore and manage the intricate details of multi-agent collaboration, necessitating more intuitive and flexible frameworks.\n",
            "\n",
            "**Applications and Frameworks**\n",
            "\n",
            "The papers collectively explore a wide range of applications for LLM-powered MAS, demonstrating their potential to transform various domains. These include:\n",
            "\n",
            "1. **Software Development and Embodied Agents**: Paper 1 highlights the use of LLM-MA systems in software development and embodied agents, where agents assume roles like programmers, testers, and robots to solve complex tasks.\n",
            "\n",
            "2. **Text Evaluation and Debates**: Paper 2 introduces ChatEval, a multi-agent framework for evaluating generated text through collaborative debates. This approach has implications for improving the quality and reliability of text evaluation tasks.\n",
            "\n",
            "3. **Legal and Policy-Making Domains**: Paper 3 proposes the use of LLM-MA systems in legal and policy-making domains, where agents can simulate courtroom scenarios or evaluate policy outcomes.\n",
            "\n",
            "4. **Blockchain and Fraud Detection**: Paper 4 explores the application of MAS in blockchain and fraud detection, where agents collaborate to analyze patterns and enhance security.\n",
            "\n",
            "5. **Gaming and Strategic Interactions**: Papers 1 and 5 discuss the use of LLM-MA systems in gaming and strategic interactions, enabling agents to engage in competitive or cooperative scenarios.\n",
            "\n",
            "6. **Creative Writing and Problem-Solving**: Paper 6 demonstrates the potential of LLM-MA systems in creative writing and problem-solving, where agents can collaborate to generate innovative solutions.\n",
            "\n",
            "The papers also propose several frameworks to address these applications, including ChatEval, CoELA, Retroformer, and AgentCoord. These frameworks provide structured approaches for designing and optimizing LLM-based MAS, addressing challenges like coordination, communication, and memory management.\n",
            "\n",
            "**Conclusion**\n",
            "\n",
            "The integration of large language models into multi-agent systems represents a significant advancement in artificial intelligence, enabling agents to collaborate, communicate, and solve complex tasks more effectively. The six papers collectively highlight the key concepts, techniques, challenges, and applications of LLM-powered MAS, providing a comprehensive overview of the current state of the field.\n",
            "\n",
            "While the potential of LLM-MA systems is immense, addressing challenges like evaluation, task allocation, memory management, and ethical concerns will be critical to realizing their full potential. The proposed frameworks and applications demonstrate the transformative power of LLMs in enhancing the capabilities of MAS, offering new opportunities for innovation across various domains.\n",
            "\n",
            "As the field continues to evolve, future research should focus on developing standardized evaluation metrics, improving the robustness and security of LLMs, and exploring new applications in areas like autonomous driving and robotics. By addressing these challenges and leveraging the unique capabilities of LLMs, multi-agent systems powered by large language models will play an increasingly important role in shaping the future of artificial intelligence.\n",
            "\n",
            "Score (BLEU or Overlap): 0.5794\n"
          ]
        }
      ],
      "source": [
        "# Agents\n",
        "summarizers = [\n",
        "    AssistantAgent(\n",
        "        name=f\"Summarizer{i+1}\", model_client=model_client_llama,\n",
        "        system_message=(\n",
        "            f\"Summarize the provided chunks from paper {i+1} into exactly 1000 words, focusing on its contributions to multi-agent systems powered by large language models (LLMs). \"\n",
        "            \"Include: (1) the paper’s main focus within multi-agent LLM research, \"\n",
        "            \"(2) key concepts or techniques (e.g., agent roles, communication, learning), \"\n",
        "            \"(3) challenges or limitations noted, and \"\n",
        "            \"(4) applications or frameworks proposed. \"\n",
        "            \"Use a clear, formal tone, staying concise and relevant to the chunks provided. \"\n",
        "            \"Ensure exactly 1000 words and end with 'TERMINATE'.\"\n",
        "        ),\n",
        "        description=f\"Generates a 1000-word summary for paper {i+1} on multi-agent LLM systems.\"\n",
        "    ) for i in range(6)\n",
        "]\n",
        "\n",
        "compiler = AssistantAgent(\n",
        "    name=\"Compiler\", model_client=model_client_deepseek,\n",
        "    system_message=(\n",
        "        \"Synthesize the six 500-word summaries into a 1000-word summary on multi-agent systems powered by large language models (LLMs). \"\n",
        "        \"Structure it with: (1) an introduction framing the collective scope of the six papers, \"\n",
        "        \"(2) a synthesis of key concepts and techniques (e.g., communication, collaboration, learning), highlighting shared or unique ideas, \"\n",
        "        \"(3) a consolidated view of challenges and limitations, noting commonalities or differences, and \"\n",
        "        \"(4) an overview of applications and frameworks, linking them across papers. \"\n",
        "        \"Create a cohesive narrative with connections (e.g., how one paper’s challenge ties to another’s solution). \"\n",
        "        \"Use a clear, formal tone, avoid repetition, and ensure exactly 1000 words. End with 'TERMINATE'.\"\n",
        "    ),\n",
        "    description=\"Compiles six 500-word summaries into a 1000-word summary on multi-agent LLM systems.\"\n",
        ")\n",
        "\n",
        "human_proxy = UserProxyAgent(\n",
        "    name=\"HumanProxy\", human_input_mode=\"ALWAYS\", max_consecutive_auto_reply=0,\n",
        "    code_execution_config={\"use_docker\": False}, description=\"Human overseer.\"\n",
        ")\n",
        "\n",
        "# Teams\n",
        "teams = [RoundRobinGroupChat([summarizers[i], human_proxy], termination_condition=MaxMessageTermination(max_messages=3)) for i in range(6)]\n",
        "team_compiler = RoundRobinGroupChat([compiler, human_proxy], termination_condition=MaxMessageTermination(max_messages=3))\n",
        "\n",
        "# Load and chunk papers (fixed for list of lists)\n",
        "all_chunks = []\n",
        "for i in range(1, 7):\n",
        "    try:\n",
        "        text = extract_text_from_pdf(f\"paper{i}.pdf\")\n",
        "        paper_chunks = chunk_text(text)\n",
        "        all_chunks.append(paper_chunks)\n",
        "        print(f\"Extracted {len(paper_chunks)} chunks from paper{i}.pdf\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing paper{i}.pdf: {e}\")\n",
        "\n",
        "async def process_paper_chunks(team, chunks, paper_idx, summarizer_name):\n",
        "    paper_chunks = chunks  # chunks is already the paper's chunk list\n",
        "    task = f\"Process paper {paper_idx+1}:\\n\\n\" + \"\\n\\n\".join(f\"Chunk {i+1}: {chunk[:500]}...\" for i, chunk in enumerate(paper_chunks))\n",
        "    print(f\"Processing paper {paper_idx+1} with {len(paper_chunks)} chunks...\")\n",
        "    logging.info(f\"Started processing paper {paper_idx+1}\")\n",
        "    async for msg in team.run_stream(task=task):\n",
        "        if hasattr(msg, 'source') and msg.source == summarizer_name:\n",
        "            content = msg.content.strip()\n",
        "            if content and \"TERMINATE\" in content:\n",
        "                word_count = count_words(content.replace('TERMINATE', ''))\n",
        "                print(f\"Word count: {word_count}\")\n",
        "                logging.info(f\"Summary for paper {paper_idx+1}: {content}\")\n",
        "                return content.replace('TERMINATE', '').strip()\n",
        "    print(f\"Warning: No summary generated for paper {paper_idx+1}\")\n",
        "    return None\n",
        "\n",
        "async def main_step1():\n",
        "    summaries = []\n",
        "    for i, team in enumerate(teams):\n",
        "        summary = await process_paper_chunks(team, all_chunks[i], i, f\"Summarizer{i+1}\")\n",
        "        summaries.append(summary)\n",
        "    return summaries\n",
        "\n",
        "# Rest of the code remains the same\n",
        "\n",
        "async def compile_summaries(team, summaries_500):\n",
        "    task = f\"Compile these summaries:\\n\\n\" + \"\\n\\n\".join(f\"Summary {i+1}: {s}\" for i, s in enumerate(summaries_500))\n",
        "    print(\"Compiling into 1000-word summary...\")\n",
        "    logging.info(\"Started compiling 1000-word summary\")\n",
        "    async for msg in team.run_stream(task=task):\n",
        "        if hasattr(msg, 'source') and msg.source == 'Compiler':\n",
        "            content = msg.content.strip()\n",
        "            if content and \"TERMINATE\" in content:\n",
        "                word_count = count_words(content.replace('TERMINATE', ''))\n",
        "                print(f\"Word count: {word_count}\")\n",
        "                logging.info(f\"Final summary: {content}\")\n",
        "                return content.replace('TERMINATE', '').strip()\n",
        "    return None\n",
        "\n",
        "def evaluate_output(system_summary, manual_summary):\n",
        "    try:\n",
        "        system_words = system_summary.split()\n",
        "        manual_words = manual_summary.split()\n",
        "        bleu_score = sentence_bleu([manual_words], system_words)\n",
        "    except TypeError:\n",
        "        system_set = set(system_summary.split())\n",
        "        manual_set = set(manual_summary.split())\n",
        "        overlap = len(system_set & manual_set) / len(manual_set) if manual_set else 0\n",
        "        bleu_score = overlap\n",
        "    return bleu_score\n",
        "\n",
        "async def main():\n",
        "    logging.info(\"Starting system test with 6 papers\")\n",
        "    summaries_500 = await main_step1()\n",
        "    final_summary = await compile_summaries(team_compiler, summaries_500)\n",
        "    print(\"\\nFinal 1000-Word Summary:\")\n",
        "    print(final_summary)\n",
        "    \n",
        "    # Placeholder for manual review (add below)\n",
        "    manual_review = \"\"\"\n",
        "            **Introduction**\n",
        "            The field of multi-agent systems powered by large language models (LLMs) has witnessed significant growth in recent years. This collective scope of research has explored the potential of LLMs in enabling agents to collaboratively engage in planning, discussions, and decision-making. This summary synthesizes the key concepts, techniques, challenges, and applications presented in six papers, highlighting shared ideas, unique contributions, and connections across papers.\n",
        "            **Key Concepts and Techniques**\n",
        "            The papers collectively highlight the importance of various concepts and techniques in LLM-based multi-agent systems. These include:\n",
        "            * Agent profiling, communication, and capability acquisition (Paper 1)\n",
        "            * Agent roles, communication, and learning in debate-style conversations (Paper 2)\n",
        "            * Collaboration, division of labor, and adaptation in multi-agent systems (Paper 3)\n",
        "            * Memory management, game theory, and task allocation in multi-agent systems (Paper 4)\n",
        "            * Language-conditioned reinforcement learning and explicit inter-agent communication (Paper 5)\n",
        "            * Visual exploration frameworks for designing coordination strategies (Paper 6)\n",
        "            These concepts and techniques demonstrate the potential of LLMs in enhancing the capabilities of multi-agent systems, facilitating collaboration, and improving decision-making.\n",
        "            **Challenges and Limitations**\n",
        "            The papers also identify several challenges and limitations in LLM-based multi-agent systems, including:\n",
        "            * Aligning LLM-MA systems with operational environments and collective objectives (Paper 1)\n",
        "            * Addressing issues related to fairness, diversity, and explainability in LLM-based evaluators (Paper 2)\n",
        "            * Careful design and oversight to ensure ethical operation and decision-making (Paper 3)\n",
        "            * Optimizing task allocation, managing complex context information, and fostering robust reasoning (Paper 4)\n",
        "            * Integrating LLMs with multi-agent reinforcement learning and addressing open research problems (Paper 5)\n",
        "            * Cognitive overload and flexibility in visual exploration frameworks (Paper 6)\n",
        "            These challenges and limitations underscore the need for further research to address the complexities and limitations of LLM-based multi-agent systems.\n",
        "            **Applications and Frameworks**\n",
        "            The papers propose various applications and frameworks for LLM-based multi-agent systems, including:\n",
        "            * Software development, world simulation, psychology, collaboration, embodied agents, debating, and policy making (Paper 1)\n",
        "            * Text evaluation, natural language generation, human-computer interaction, and cognitive science (Paper 2)\n",
        "            * Oracle agents, system designer agents, plugins, and multi-agent collaboration (Paper 3)\n",
        "            * Blockchain and fraud detection (Paper 4)\n",
        "            * Embodied systems, autonomous driving, and language-conditioned MARL for problem-solving and gaming (Paper 5)\n",
        "            * AgentCoord system, plan outline generation, agent assignment exploration, and task process view (Paper 6)\n",
        "            These applications and frameworks demonstrate the potential of LLM-based multi-agent systems to transform various domains and improve system performance.\n",
        "            **Conclusion**\n",
        "            In conclusion, this summary provides a comprehensive overview of the progress and challenges in LLM-based multi-agent systems. The papers collectively highlight the potential of LLMs in enabling agents to collaboratively engage in planning, discussions, and decision-making. While challenges and limitations exist, the proposed applications and frameworks demonstrate the potential of LLM-based multi-agent systems to transform various domains and improve system performance. Further research is needed to address the complexities and limitations of these systems, but the potential benefits are substantial.\n",
        "        \"\"\"    \n",
        "    bleu_score = evaluate_output(final_summary, manual_review)\n",
        "    print(f\"\\nScore (BLEU or Overlap): {bleu_score:.4f}\")\n",
        "    logging.info(f\"Evaluation completed. Score: {bleu_score:.4f}\")\n",
        "\n",
        "await main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
