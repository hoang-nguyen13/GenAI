{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWhQdWO_4qqe"
   },
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "  <h1></h1>\n",
    "  <h1>Stylized Retrieval-Augmented Generation</h1>\n",
    "  <h4 align=\"center\">Assignmnet II</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi69xdNwKZPv"
   },
   "source": [
    "Welcome to Assignment II! In this notebook, you will build and implement a Retrieval-Augmented Generation (RAG) pipeline tailored for a text style transfer application.\n",
    "\n",
    "**By the end of this assignment, you'll be able to:**\n",
    "\n",
    "*   Build a Retrieval-Augmented Generation (RAG) pipeline to enhance text generation with external knowledge.\n",
    "*   Retrieve relevant information from a dataset or knowledge base to support text generation.\n",
    "*   Implement a neural style transfer model to transform text into a desired writing style.\n",
    "*   Combine retrieved content and style transfer to create a coherent and stylistically customized output.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUWduwxuKgBa"
   },
   "source": [
    "## Important Note on Submission\n",
    "\n",
    "\n",
    "*   Do not use ChatGPT or any other AI tool to directly produce the code. If you need assistance, refer to Exercise 5 for guidance.\n",
    "*   You are allowed to work in a group of up to 3 members.\n",
    "*   Do not copy code or answers from other groups. Collaboration is encouraged only within your own group.\n",
    "*   Ensure that your notebook is runnable without any errors. Submissions with errors will not be accepted.\n",
    "*   Answers to open-ended questions must be original and not copied from other groups or AI tools like ChatGPT.\n",
    "*   The submission should be one .ipynb notebook with the group members' names on Openlat and matriculation numbers on it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_gXYEhB7WzW"
   },
   "source": [
    "## Group Members\n",
    "\n",
    "\n",
    "1. First memebr:\n",
    "  * Name:\n",
    "  * Matrikel-Nr.:\n",
    "2. Second memebr:\n",
    "  * Name:\n",
    "  * Matrikel-Nr.:\n",
    "2. Third memebr:\n",
    "  * Name:\n",
    "  * Matrikel-Nr.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bss2qVkJKkZr"
   },
   "source": [
    "### Table of Contents\n",
    "- [1. Access to Hugging Face](#1-access-to-hugging-face)\n",
    "- [2. Packages](#2-packages)\n",
    "- [3. Problem Statement](#3-problem-statement)\n",
    "- [4. Fetch and Parse](#4-fetch-and-parse)\n",
    "- [5. Calculate Word Stats](#5-calculate-word-stats)\n",
    "- [6. Set Up LLM](#6-set-up-llm)\n",
    "- [7. BM25 Retriever](#7-bm25-retriever)\n",
    "- [8. Build Chroma](#8-build-chroma)\n",
    "- [9. Ensemble Retriever](#9-ensemble-retriever)\n",
    "- [10. Format Documents](#10-format-documents)\n",
    "- [11. RAG Chain](#11-rag-chain)\n",
    "- [12. Final Response](#12-final-response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZWrcJ4LKneE"
   },
   "source": [
    "# 1. Access to Hugging face\n",
    "Execute the following cell to connect to your Hugging Face account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pe3-F-kiKYiD"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Prompt user for Hugging Face API token if not already set\n",
    "if \"HUGGINGFACEHUB_API_TOKEN\" not in os.environ:\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your Huggingfacehub API token: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N72UZSMKqSa"
   },
   "source": [
    "# 2. Packages\n",
    "Execute the following code cells for installing the packages needed for creating your Stylized RAG.\n",
    "\n",
    "note: If there are package conflics you can use pip-tools to automatically find and install the compatible versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WLei89zCKsEj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.3.14-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.36-cp311-cp311-macosx_10_9_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.11.11-cp311-cp311-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.29 (from langchain)\n",
      "  Using cached langchain_core-0.3.29-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.3,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.2.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy<2,>=1.22.4 (from langchain)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain) (2.10.5)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain) (9.0.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading propcache-0.2.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-macosx_10_9_x86_64.whl.metadata (69 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.29->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.14)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.3,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
      "Using cached langchain-0.3.14-py3-none-any.whl (1.0 MB)\n",
      "Downloading aiohttp-3.11.11-cp311-cp311-macosx_10_9_x86_64.whl (468 kB)\n",
      "Using cached langchain_core-0.3.29-py3-none-any.whl (411 kB)\n",
      "Using cached langchain_text_splitters-0.3.5-py3-none-any.whl (31 kB)\n",
      "Using cached langsmith-0.2.10-py3-none-any.whl (326 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.36-cp311-cp311-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-macosx_10_9_x86_64.whl (54 kB)\n",
      "Downloading greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl (272 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-macosx_10_9_x86_64.whl (29 kB)\n",
      "Downloading propcache-0.2.1-cp311-cp311-macosx_10_9_x86_64.whl (45 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-macosx_10_9_x86_64.whl (94 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: propcache, numpy, multidict, jsonpointer, greenlet, frozenlist, attrs, aiohappyeyeballs, yarl, SQLAlchemy, requests-toolbelt, jsonpatch, aiosignal, langsmith, aiohttp, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.1\n",
      "    Uninstalling numpy-2.2.1:\n",
      "      Successfully uninstalled numpy-2.2.1\n",
      "Successfully installed SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 attrs-24.3.0 frozenlist-1.5.0 greenlet-3.1.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.14 langchain-core-0.3.29 langchain-text-splitters-0.3.5 langsmith-0.2.10 multidict-6.1.0 numpy-1.26.4 propcache-0.2.1 requests-toolbelt-1.0.0 yarl-1.18.3\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-community) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-community) (3.11.11)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-community) (0.3.14)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-community) (0.3.29)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-community) (0.2.10)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-community) (1.26.4)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.24.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (0.3.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (2.10.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (2.27.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
      "Using cached langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
      "Downloading marshmallow-3.24.2-py3-none-any.whl (49 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.14 marshmallow-3.24.2 mypy-extensions-1.0.0 pydantic-settings-2.7.1 typing-inspect-0.9.0\n",
      "Collecting langchain-huggingface\n",
      "  Using cached langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-huggingface) (0.27.1)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-huggingface) (0.3.29)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n",
      "  Using cached sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-huggingface) (0.21.0)\n",
      "Collecting transformers>=4.39.0 (from langchain-huggingface)\n",
      "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.2.10)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (9.0.0)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting scikit-learn (from sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Downloading scikit_learn-1.6.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (31 kB)\n",
      "Collecting scipy (from sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Downloading scipy-1.15.0-cp311-cp311-macosx_14_0_x86_64.whl.metadata (61 kB)\n",
      "Collecting Pillow (from sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Downloading pillow-11.1.0-cp311-cp311-macosx_10_10_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from transformers>=4.39.0->langchain-huggingface) (1.26.4)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.39.0->langchain-huggingface)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.39.0->langchain-huggingface)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2024.12.14)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.3)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.14.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.3.1)\n",
      "Using cached langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Using cached sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Using cached transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl (287 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-macosx_10_12_x86_64.whl (427 kB)\n",
      "Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.1.0-cp311-cp311-macosx_10_10_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.0-cp311-cp311-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.0-cp311-cp311-macosx_14_0_x86_64.whl (27.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-macosx_10_9_universal2.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, safetensors, regex, Pillow, networkx, MarkupSafe, joblib, scikit-learn, jinja2, torch, transformers, sentence-transformers, langchain-huggingface\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.1.0 jinja2-3.1.5 joblib-1.4.2 langchain-huggingface-0.1.2 networkx-3.4.2 regex-2024.11.6 safetensors-0.5.2 scikit-learn-1.6.0 scipy-1.15.0 sentence-transformers-3.3.1 threadpoolctl-3.5.0 torch-2.2.2 transformers-4.47.1\n",
      "Collecting bs4\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.6\n",
      "Collecting rank_bm25\n",
      "  Using cached rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from rank_bm25) (1.26.4)\n",
      "Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (0.27.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests->huggingface_hub) (2024.12.14)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-community\n",
    "!pip install langchain-huggingface\n",
    "!pip install bs4\n",
    "!pip install rank_bm25\n",
    "!pip install huggingface_hub\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-chroma\n",
      "  Using cached langchain_chroma-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0 (from langchain-chroma)\n",
      "  Using cached chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-chroma) (0.115.6)\n",
      "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-chroma) (0.3.29)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.22.4 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-chroma) (1.26.4)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.10.5)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.7.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.34.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.7.5)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.20.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.29.0)\n",
      "Collecting tokenizers<=0.20.3,>=0.13.2 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading tokenizers-0.20.3-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.69.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.10.14)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (13.9.4)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.41.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma) (0.2.10)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.2.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (4.8.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.37.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.8.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.9)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma) (1.0.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (24.12.23)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (5.29.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.13.3)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.29.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.29.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.50b0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.50b0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.50b0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.50b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.17.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.27.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.0.3)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (14.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (4.9)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (2024.12.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (3.4.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0->langchain-chroma) (0.6.1)\n",
      "Downloading langchain_chroma-0.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-macosx_10_12_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, chromadb, langchain-chroma\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: chromadb\n",
      "    Found existing installation: chromadb 0.6.2\n",
      "    Uninstalling chromadb-0.6.2:\n",
      "      Successfully uninstalled chromadb-0.6.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chromadb-0.5.23 langchain-chroma-0.2.0 tokenizers-0.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.46.0\n",
      "  Downloading transformers-4.46.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from transformers==4.46.0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from transformers==4.46.0) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from transformers==4.46.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from transformers==4.46.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from transformers==4.46.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from transformers==4.46.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from transformers==4.46.0) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from transformers==4.46.0) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from transformers==4.46.0) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from transformers==4.46.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests->transformers==4.46.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests->transformers==4.46.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests->transformers==4.46.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/python311_env/lib/python3.11/site-packages (from requests->transformers==4.46.0) (2024.12.14)\n",
      "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'transformers' candidate (version 4.46.0 at https://files.pythonhosted.org/packages/db/88/1ef8a624a33d7fe460a686b9e0194a7916320fc0d67d4e38e570beeac039/transformers-4.46.0-py3-none-any.whl (from https://pypi.org/simple/transformers/) (requires-python:>=3.8.0))\n",
      "Reason for being yanked: This version unfortunately does not work with 3.8 but we did not drop the support yet\u001b[0m\u001b[33m\n",
      "\u001b[0mDownloading transformers-4.46.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.1\n",
      "    Uninstalling transformers-4.47.1:\n",
      "      Successfully uninstalled transformers-4.47.1\n",
      "Successfully installed transformers-4.46.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.46.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2K3p5lXKuSy"
   },
   "source": [
    "# 3. Problem Statement\n",
    "In this assignment, we will implement **Text Style Transfer**, a technique that modifies text style while preserving its content. They will build an **ensemble retriever** combining **BM25** for keyword-based retrieval and **Chroma** for semantic search to retrieve relevant documents, which will be used as input for the style transfer process. This project integrates classical retrieval methods with modern neural embeddings for practical NLP applications.\n",
    "\n",
    "**what is text style transfer?**\n",
    "\n",
    "**Text Style Transfer** is a natural language processing (NLP) technique that modifies the style of a given text while preserving its original content. It allows for the transformation of linguistic expressions to convey different tones, emotions, or writing styles without altering the underlying meaning. For example, it can rephrase formal text into a casual tone, adapt neutral statements into an emotional tone, or convert modern language into a Shakespearean style. This technique has applications in personalized communication, creative writing, sentiment adjustment, and even domain adaptation, making it a powerful tool for generating diverse textual outputs tailored to specific needs.\n",
    "\n",
    "### Example of Text Style Transfer:\n",
    "\n",
    "#### **Input (Neutral Tone):**\n",
    "\"I am excited about the opportunity to work on this project.\"\n",
    "\n",
    "#### **Output (Formal Tone):**\n",
    "\"I am genuinely enthusiastic about the prospect of contributing to this project.\"\n",
    "\n",
    "#### **Output (Casual Tone):**\n",
    "\"I'm super pumped to get started on this project!\"\n",
    "\n",
    "#### **Output (Shakespearean Style):**\n",
    "\"Verily, I am thrilled by the chance to partake in this noble endeavor.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQc6rEV4MHfY"
   },
   "source": [
    "# 4. Fetch and Parse\n",
    "In this part of the assignment, you are tasked with:\n",
    "\n",
    "*    Fetching and parsing web content: Write a function that fetches the HTML content of a webpage and processes it to extract clean, readable text.\n",
    "*    Splitting text into smaller chunks: Implement a function to split the text into overlapping chunks, ensuring that each chunk is manageable for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "0kxayMljLPhu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "def fetch_and_parse(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the webpage content at `url` and return a cleaned string of text.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL of the webpage to fetch.\n",
    "\n",
    "    Returns:\n",
    "    - str: Cleaned text content extracted from the webpage.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Fetch the webpage content using the requests library.\n",
    "    # Fetch the content of the URL.\n",
    "    # Ensure the request is successful.\n",
    "    \n",
    "    # Step 2: Parse the HTML content using BeautifulSoup.\n",
    "\n",
    "    # Step 3: Extract the text content from the parsed HTML.\n",
    "\n",
    "    # Step 4: Return the cleaned text.\n",
    "\n",
    "    # Write your code here.\n",
    "    try:\n",
    "        # Step 1\n",
    "        fetch = requests.get(url, timeout = 10)\n",
    "        if fetch.status_code != 200:\n",
    "            print(f\"Failed to fetch URL: {url} with status code: {fetch.status_code}\")\n",
    "            return \"\"\n",
    "        # Step 2\n",
    "        soup = BeautifulSoup(fetch.text, 'html.parser')\n",
    "        # Step 3\n",
    "        text = soup.get_text(separator=\"\\n\").strip() #implement clean text here\n",
    "        cleaned_text = \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
    "        # Step 4\n",
    "        return cleaned_text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def split_text_into_documents(text: str, chunk_size: int = 1000, chunk_overlap: int = 100):\n",
    "    \"\"\"\n",
    "    Split a long text into overlapping chunks and return them as a list of Documents.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The long text to split.\n",
    "    - chunk_size (int): The size of each chunk (default is 1000 characters).\n",
    "    - overlap (int): The number of overlapping characters between consecutive chunks (default is 100).\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of Documents, each containing a chunk of text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store the chunks.\n",
    "    docs = []\n",
    "        \n",
    "    # Write your code here.\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = splitter.create_documents([text])\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC4W_opcOi1o"
   },
   "source": [
    "1. Why do we split the text into smaller chunks before storing or processing it?\n",
    "\n",
    "Answer:\n",
    "\n",
    "2. What challenges might you face when fetching and parsing web content, and how would you handle them?\n",
    "\n",
    "Answer:\n",
    "\n",
    "3. In the context of RAG, how would errors in the fetch_and_parse function affect the overall pipeline?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmZQ8HhGPugo"
   },
   "source": [
    "# 5. Calculate Word Stats\n",
    "\n",
    "In this task, you will implement a function to calculate basic word and character statistics for a list of documents. Each document is represented as a Document object with a page_content attribute that contains its text.\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Calculate the total number of words and characters across all documents.\n",
    "2. Compute the average number of words and characters per document.\n",
    "3. Print the average statistics in a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "BkmqrEeYM-8z"
   },
   "outputs": [],
   "source": [
    "def calculate_word_stats(texts):\n",
    "    \"\"\"\n",
    "    Calculate and display average word and character statistics for a list of documents.\n",
    "\n",
    "    Parameters:\n",
    "    - texts (list): A list of Document objects, where each Document contains a `page_content` attribute.\n",
    "\n",
    "    Returns:\n",
    "    - None: Prints the average word and character counts per document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Initialize variables to keep track of total words and total characters.\n",
    "    total_words, total_characters = 0, 0\n",
    "\n",
    "    # Step 2: Iterate through each document in the `texts` list.\n",
    "    for doc in texts:\n",
    "        content = doc.page_content\n",
    "        word_count = len(content.split())\n",
    "        char_count = len(content)\n",
    "        total_words += word_count\n",
    "        total_characters += char_count\n",
    "\n",
    "    # Step 3: Calculate the average words and characters per document.\n",
    "    # - Avoid division by zero by checking if the `texts` list is not empty.\n",
    "    num_docs = len(texts)\n",
    "    avg_words = total_words / num_docs if num_docs > 0 else 0\n",
    "    avg_characters = total_characters / num_docs if num_docs > 0 else 0\n",
    "\n",
    "    # Step 4: Print the calculated averages in a readable format.\n",
    "    # Example: \"Average words per document: 123.45\"\n",
    "    print(f\"Average words per document: {avg_words}\")\n",
    "    print(f\"Average characters per document: {avg_characters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "EpNcNKHdh_ar"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average words per document: 7.75\n",
      "Average characters per document: 44.5\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell to test your calculate_word_stats function.\n",
    "# Create sample Document objects with text content for testing your code above.\n",
    "sample_docs = [\n",
    "    Document(page_content=\"This is the first test document.\"),\n",
    "    Document(page_content=\"Here is another example document for testing.\"),\n",
    "    Document(page_content=\"Short text.\"),\n",
    "    Document(page_content=\"This document has more content. It's longer and has more words in it for testing purposes.\"),\n",
    "]\n",
    "\n",
    "# Call the function with the sample documents to calculate word statistics.\n",
    "calculate_word_stats(sample_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zzYkovNisNP"
   },
   "source": [
    "1. What potential issues could arise if the texts list is empty or contains documents with no content, and how would you address them?\n",
    "\n",
    "Answer:\n",
    "\n",
    "2. Why is it beneficial to calculate both word count and character count instead of just one of them?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIIdfz7tlPdG"
   },
   "source": [
    "# 6. Set Up LLM\n",
    "\n",
    "In this part of the assignment, you will implement a function to set up a Large Language Model (LLM) using the Hugging Face Endpoint API. This function will:\n",
    "\n",
    "1. Initialize and connect to a pre-trained model available on Hugging Face.\n",
    "2. Allow customization of parameters like the model repository ID and generation temperature.\n",
    "3. Return the configured LLM object, which will be used later for text generation tasks in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "GnwtA4xolKWt"
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "def setup_llm(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    \"\"\"\n",
    "    Set up and return a Hugging Face LLM using the specified model repository ID and generation parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - repo_id (str): The repository ID of the Hugging Face model to use (default: \"mistralai/Mistral-7B-Instruct-v0.3\").\n",
    "    - temperature (float): The generation temperature to control creativity in outputs (default: 1.0).\n",
    "\n",
    "    Returns:\n",
    "    - HuggingFaceEndpoint: A configured LLM object ready for text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Import the HuggingFaceEndpoint class.\n",
    "    # - This class allows you to connect to a Hugging Face model hosted on an endpoint.\n",
    "\n",
    "    # Step 2: Configure the LLM connection.\n",
    "    # - Use the HuggingFaceEndpoint class to set up the LLM.\n",
    "\n",
    "    # Step 3: Return the configured LLM object.\n",
    "    # - The returned LLM can be used for generating text based on input prompts.\n",
    "\n",
    "    # Write your code here.\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=repo_id,\n",
    "        temperature = 0.6,\n",
    "    )\n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQu0sETym99k"
   },
   "source": [
    "1. What would happen if the temperature is set to an extreme value (e.g., 0 or 10)? How would you prevent misuse?\n",
    "\n",
    "Answer:\n",
    "\n",
    "2. If the LLM generates incorrect or irrelevant responses, what steps would you take to diagnose and fix the issue?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdEyYy86oxn9"
   },
   "source": [
    "# 7. BM25 Retriever\n",
    "\n",
    "In this task, students will implement a BM25 Retriever, a critical component of the RAG pipeline.\n",
    "Your task is to:\n",
    "\n",
    "1. Initialize the BM25 retriever with a set of documents.\n",
    "2. Implement a method to retrieve the top k most relevant documents for a given query.\n",
    "3. Use efficient tokenization and scoring to ensure accurate and fast results.\n",
    "This component will enable the pipeline to fetch relevant information from a corpus, which is then passed to the LLM for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kp7Th24kmKLm"
   },
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "class BM25Retriever:\n",
    "    \"\"\"\n",
    "    A class to implement BM25-based document retrieval.\n",
    "\n",
    "    Attributes:\n",
    "    - documents (list): A list of Document objects.\n",
    "    - corpus (list): A list of strings representing the document contents.\n",
    "    - tokenized_corpus (list): A list of tokenized documents (lists of words).\n",
    "    - bm25 (BM25Okapi): The BM25 retriever initialized with the tokenized corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents):\n",
    "        \"\"\"\n",
    "        Initialize the BM25 retriever with the given documents.\n",
    "\n",
    "        Parameters:\n",
    "        - documents (list): A list of Document objects.\n",
    "        \"\"\"\n",
    "        # Step 1: Store the input documents.\n",
    "        # Hint: Use the `page_content` attribute of each Document object to extract text.\n",
    "        # Step 2: Tokenize the corpus.\n",
    "        # Hint: Use the `.split()` method to tokenize each document into words.\n",
    "        # Step 3: Initialize the BM25 retriever with the tokenized corpus.\n",
    "        self.documents = documents\n",
    "        self.corpus = [doc.page_content for doc in documents]\n",
    "        self.tokenized_corpus = [doc.split() for doc in self.corpus]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "\n",
    "    def retrieve(self, query, k=5):\n",
    "        \"\"\"\n",
    "        Retrieve the top `k` most relevant documents for a given query.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The input query as a string.\n",
    "        - k (int): The number of top documents to return (default is 5).\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of the top `k` relevant documents as strings.\n",
    "        \"\"\"\n",
    "        # Step 1: Tokenize the input query.\n",
    "        # Hint: Use `.split()` to tokenize the query into words.\n",
    "        # Step 2: Use the BM25 retriever to score and rank documents.\n",
    "        # Hint: Use the `bm25.get_top_n()` method to retrieve the top `k` documents.\n",
    "        # Step 3: Return the top `k` relevant documents.\n",
    "        tokenized_query = query.split()\n",
    "        bm25_k_results = self.bm25.get_top_n(tokenized_query, self.corpus, n=k)\n",
    "        return bm25_k_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skPWlylvpWgB"
   },
   "source": [
    "Execute the following code to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "qrEkVO9UpUtk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Relevant Documents:\n",
      "1. Machine learning is a method of data analysis that automates analytical model building.\n",
      "2. Natural language processing is a field of AI focused on the interaction between computers and human language.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Create sample Document objects.\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning is a method of data analysis that automates analytical model building.\"),\n",
    "    Document(page_content=\"Deep learning is a subset of machine learning that uses neural networks with three or more layers.\"),\n",
    "    Document(page_content=\"Artificial intelligence encompasses a wide range of technologies, including machine learning and deep learning.\"),\n",
    "    Document(page_content=\"Natural language processing is a field of AI focused on the interaction between computers and human language.\"),\n",
    "]\n",
    "\n",
    "# Initialize the retriever with the sample documents.\n",
    "retriever = BM25Retriever(sample_docs)\n",
    "\n",
    "# Test the retriever with a query.\n",
    "query = \"What is machine learning?\"\n",
    "top_docs = retriever.retrieve(query,k=2)\n",
    "\n",
    "# Print the results.\n",
    "print(\"Top Relevant Documents:\")\n",
    "for idx, doc in enumerate(top_docs, 1):\n",
    "    print(f\"{idx}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmdT52BtqwfQ"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "Top Relevant Documents:\n",
    "1. Machine learning is a method of data analysis that automates analytical model building.\n",
    "2. Artificial intelligence encompasses a wide range of technologies, including machine learning and deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3s42FiJ8rEOE"
   },
   "source": [
    "1. If two documents have identical content except for minor differences (e.g., synonyms or paraphrasing), how might BM25 handle this, and why?\n",
    "\n",
    "Answer:\n",
    "\n",
    "2. What challenges might arise if the corpus contains very short or very long documents? How would you address these challenges?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZhBH_Aqs2of"
   },
   "source": [
    "# 8. Build Chroma\n",
    "In this task, students will implement a function to build a Chroma vector store, a key component of the RAG pipeline. The Chroma vector store enables efficient semantic search by embedding documents into a high-dimensional vector space. Using these embeddings, the retriever can find documents that are semantically similar to a given query.\n",
    "\n",
    "The task involves:\n",
    "\n",
    "1. Initializing a vector store (Chroma) with Hugging Face embeddings.\n",
    "2. Adding a list of documents to the vector store.\n",
    "3. Returning the vector store for later use in the retrieval and generation pipeline.\n",
    "\n",
    "This function sets up the semantic retrieval system, allowing for more meaningful and context-aware results than keyword-based retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "YiAtoavjpgPt"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "def build_chroma(documents: list[Document]) -> Chroma:\n",
    "    \"\"\"\n",
    "    Build a Chroma vector store using Hugging Face embeddings\n",
    "    and add the documents to it.\n",
    "\n",
    "    Parameters:\n",
    "    - documents (list[Document]): A list of Document objects to add to the vector store.\n",
    "\n",
    "    Returns:\n",
    "    - Chroma: The Chroma vector store containing the embedded documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Initialize Hugging Face embeddings.\n",
    "    # - Use a pre-trained embedding model (e.g., \"sentence-transformers/all-mpnet-base-v2\").\n",
    "    # - HuggingFaceEmbeddings generates dense vector representations for text.\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "    # Step 2: Initialize the Chroma vector store.\n",
    "    # - Set the collection name for the vector store (e.g., \"EngGenAI\").\n",
    "    # - Pass the Hugging Face embeddings as the embedding function.\n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"EngGenAI\",\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "\n",
    "    # Step 3: Add the input documents to the Chroma vector store.\n",
    "    # - Use the `add_documents` method to embed and store the documents.\n",
    "    vector_store.add_documents(documents=documents)\n",
    "    # Step 4: Return the Chroma vector store for later use.\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivy_VQFItWTr"
   },
   "source": [
    "Execute the following code to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "-0Vb-uDutN3-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store built successfully!\n",
      "<langchain_community.vectorstores.chroma.Chroma object at 0x130455b90>\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Create sample Document objects.\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning is a method of data analysis that automates analytical model building.\"),\n",
    "    Document(page_content=\"Deep learning is a subset of machine learning that uses neural networks with three or more layers.\"),\n",
    "    Document(page_content=\"Artificial intelligence encompasses a wide range of technologies, including machine learning and deep learning.\"),\n",
    "    Document(page_content=\"Natural language processing is a field of AI focused on the interaction between computers and human language.\"),\n",
    "]\n",
    "\n",
    "# Call the function to build the Chroma vector store.\n",
    "vector_store = build_chroma(sample_docs)\n",
    "\n",
    "# Test retrieval (optional, if supported).\n",
    "print(\"Vector store built successfully!\")\n",
    "print(vector_store)  # Print the vector store object to verify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TgIqUZGttLE"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "Vector store built successfully!\n",
    "\n",
    "<langchain.vectorstores.Chroma object at 0x7f8c1a4b3f10>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfDwKvbcvHvN"
   },
   "source": [
    "1. What happens if two documents have identical embeddings? How would you handle this in the retrieval process?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgxRBNaxv-ro"
   },
   "source": [
    "# 9. Ensemble Retriever\n",
    "\n",
    "In this task, students will implement an Ensemble Retriever that combines the strengths of Chroma (semantic similarity) and BM25 (keyword-based retrieval) to create a hybrid retriever. This ensemble approach ensures more robust and comprehensive retrieval results by leveraging both semantic and lexical search techniques.\n",
    "\n",
    "You should:\n",
    "\n",
    "1. Retrieve documents from both Chroma (semantic search) and BM25 (lexical search).\n",
    "2. Combine the results from both retrievers while deduplicating overlapping results.\n",
    "3. Return the top k most relevant and unique documents.\n",
    "\n",
    "This function plays a vital role in the RAG pipeline by ensuring that the retrieved documents are relevant and diverse, combining semantic understanding with precise keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "GYdmjXWFtxca"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "class EnsembleRetriever:\n",
    "    \"\"\"\n",
    "    Merges results from Chroma similarity search and BM25 lexical search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chroma_store, bm25_retriever):\n",
    "        \"\"\"\n",
    "        Initialize the EnsembleRetriever with Chroma and BM25 retrievers.\n",
    "\n",
    "        Parameters:\n",
    "        - chroma_store: The Chroma vector store for semantic retrieval.\n",
    "        - bm25_retriever: The BM25 retriever for lexical retrieval.\n",
    "        \"\"\"\n",
    "        # Step 1: Store the Chroma vector store and BM25 retriever.\n",
    "        # Hint: Assign the inputs `chroma_store` and `bm25_retriever` to instance variables.\n",
    "        self.chroma_store = chroma_store  # Replace with your implementation.\n",
    "        self.bm25_retriever = bm25_retriever  # Replace with your implementation.\n",
    "\n",
    "    def get_relevant_documents(self, query: str, k: int = 5):\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents by combining results from Chroma and BM25.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The input search query.\n",
    "        - k (int): The number of top unique documents to return (default: 5).\n",
    "\n",
    "        Returns:\n",
    "        - list[Document]: A list of unique relevant documents.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Step 1: Retrieve top-k documents from Chroma (semantic similarity).\n",
    "        chroma_docs =  self.chroma_store.similarity_search(query, k=k) # Replace with your implementation.\n",
    "\n",
    "        # Step 2: Retrieve top-k documents from BM25 (lexical matching).\n",
    "        bm25_docs =  self.bm25_retriever.retrieve(query, k=k) # Replace with your implementation.\n",
    "\n",
    "        # Step 3: Combine results from both retrievers into a single list.\n",
    "        combined = chroma_docs + bm25_docs  # Replace with your implementation.\n",
    "\n",
    "        # Step 4: Deduplicate the combined results.\n",
    "        # Hint: Use a `set` to track seen content based on document text.\n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        for doc in combined:\n",
    "            # Retrieve content for deduplication (check if `page_content` exists).\n",
    "            # Hint: Use `doc.page_content` if it's a Document object; otherwise, use `doc` as is.\n",
    "            if isinstance(doc, Document):\n",
    "                content = doc.page_content  # Replace with your implementation.\n",
    "            elif isinstance(doc, str):\n",
    "                content = doc\n",
    "            else:\n",
    "                raise ValueError(\"Nothing is expected.\")\n",
    "            # Use the first 60 characters of the document text as a key for deduplication.\n",
    "            key = content[:60]  # Replace with your implementation.\n",
    "\n",
    "            if key not in seen:\n",
    "                # Convert plain strings to Document objects if necessary.\n",
    "                # Hint: Use `Document(page_content=doc)` for plain text.\n",
    "                if isinstance(doc, str):\n",
    "                    doc = Document(page_content=doc)  # Replace with your implementation.\n",
    "                unique_docs.append(doc)\n",
    "                seen.add(key)\n",
    "\n",
    "        # Step 5: Return the top-k unique documents.\n",
    "        return unique_docs[:k]  # Replace with your implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86StDSBJwzYj"
   },
   "source": [
    "Run the following code to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz8G5gThtxZf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Retrieval Results:\n",
      "1. Machine learning automates model building using data.\n",
      "2. Deep learning is a type of machine learning using neural networks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Sample documents\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning automates model building using data.\"),\n",
    "    Document(page_content=\"Deep learning is a type of machine learning using neural networks.\"),\n",
    "    Document(page_content=\"AI includes technologies like machine learning and deep learning.\"),\n",
    "    Document(page_content=\"Natural language processing focuses on human-computer language interaction.\"),\n",
    "]\n",
    "\n",
    "# Sample Chroma and BM25 retrievers (mock behavior)\n",
    "class MockChroma:\n",
    "    def similarity_search(self, query, k):\n",
    "        return [Document(page_content=\"Machine learning automates model building using data.\")]\n",
    "\n",
    "class MockBM25:\n",
    "    def retrieve(self, query, k):\n",
    "        return [\"Deep learning is a type of machine learning using neural networks.\"]\n",
    "\n",
    "# Initialize mock retrievers\n",
    "chroma = MockChroma()\n",
    "bm25 = MockBM25()\n",
    "\n",
    "# Initialize EnsembleRetriever\n",
    "ensemble_retriever = EnsembleRetriever(chroma, bm25)\n",
    "\n",
    "# Test the retriever with a query\n",
    "query = \"What is machine learning?\"\n",
    "results = ensemble_retriever.get_relevant_documents(query, k=3)\n",
    "\n",
    "# Print the results\n",
    "print(\"Ensemble Retrieval Results:\")\n",
    "for idx, doc in enumerate(results, 1):\n",
    "    print(f\"{idx}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5atP_1k5wmr9"
   },
   "source": [
    "Ensemble Retrieval Results:\n",
    "1. Machine learning automates model building using data.\n",
    "2. Deep learning is a type of machine learning using neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jp3i6YimxSEl"
   },
   "source": [
    "1. Why is it beneficial to combine semantic retrieval (Chroma) and lexical retrieval (BM25) in an Ensemble Retriever?\n",
    "\n",
    "Answer:\n",
    "\n",
    "2. If the results from Chroma and BM25 are drastically different (little to no overlap), how might this impact the quality of the combined results?\n",
    "\n",
    "Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "Lqc0U7qgtwKs"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "\n",
    "class StrOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpqUJMc2yzS-"
   },
   "source": [
    "# 10. Format Documents\n",
    "\n",
    "In this task, you will implement two key components to enhance the formatting and styling of documents in the RAG pipeline:\n",
    "\n",
    "format_docs(docs):\n",
    "\n",
    "This function takes a list of documents (docs) and formats them into a readable, numbered list. If no documents are provided, it returns a default message indicating the absence of context.\n",
    "\n",
    "style_prompt:\n",
    "\n",
    "This is a prompt template that prepares the input for a neural style transfer task. It asks an AI model to rewrite a given text (original_text) in a specified style, optionally using a contextual snippet (context) from the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "3r_YDxMozMZQ"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Format a list of documents into a numbered, readable string.\n",
    "\n",
    "    Parameters:\n",
    "    - docs (list[Document]): A list of Document objects to format.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string containing the formatted documents or a default message if no documents are provided.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Check if the list of documents is empty.\n",
    "    # Hint: If `docs` is empty, return the string \"No relevant context found.\"\n",
    "    if not docs:\n",
    "        return \"No relevant context found.\"  # Replace with your implementation.\n",
    "\n",
    "    # Step 2: Initialize an empty list to store formatted snippets.\n",
    "    snippet_list = []\n",
    "\n",
    "    # Step 3: Iterate over the documents and format each one.\n",
    "    # - Use `enumerate` to get the index and document.\n",
    "    # - Extract and clean the `page_content` of the document.\n",
    "    # - Replace newlines with spaces and remove unnecessary whitespace.\n",
    "    # - Add a formatted string to the `snippet_list` (e.g., \"1. Cleaned content\").\n",
    "    for i, doc in enumerate(docs):\n",
    "        text = doc.page_content\n",
    "        formatted_string = text.replace('\\n', ' ').replace(' ','')\n",
    "        snippet_list.append(formatted_string)\n",
    "\n",
    "    # Step 4: Join the snippets with newline characters and return the result.\n",
    "    snippet = \"\\n\".join(snippet_list)\n",
    "    return snippet  # Replace with your implementation.\n",
    "\n",
    "\n",
    "# Define the style transfer prompt template\n",
    "style_prompt = PromptTemplate(\n",
    "    input_variables=[\"style\", \"context\", \"original_text\"],\n",
    "    template=(\n",
    "            # Replace with your prompt for changing the style of the text. Avoid using complicated prompts.\n",
    "            \"rewrite the original text: {original_text} with {style} style using the context: {context}\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ58VijEzuPH"
   },
   "source": [
    "Execute the following code to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "Ld8RnXL9zmnw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Documents:\n",
      "\n",
      "Machinelearningautomatesdataanalysis.\n",
      "Deeplearningusesneuralnetworkstolearnpatterns.\n",
      "Artificialintelligenceincludesvarioustechnologies.\n",
      "\n",
      "Generated Prompt for Style Transfer:\n",
      "\n",
      "rewrite the original text: Artificial intelligence is transforming the world. with poetic style using the context: Machinelearningautomatesdataanalysis.\n",
      "Deeplearningusesneuralnetworkstolearnpatterns.\n",
      "Artificialintelligenceincludesvarioustechnologies.\n",
      "\n",
      "--- Rewritten (Styled) Text ---\n",
      "\n",
      "Machinelearninganddeeplearningarepartofartificialintelligence.\n",
      "\n",
      "In a world that's swiftly evolving,\n",
      "Artificial Intelligence is the new revolution.\n",
      "It's the brainchild of our ambition,\n",
      "A creation born from human vision.\n",
      "\n",
      "Machine Learning, a key component,\n",
      "Automates data analysis, no need for repetition.\n",
      "It sifts through information,\n",
      "In search of patterns, hidden in the ocean.\n",
      "\n",
      "Deep Learning, a sibling, not a clone,\n",
      "Uses neural networks to learn patterns alone.\n",
      "It's like a sponge that soaks up the data,\n",
      "Transforming it into wisdom, no need to fret ya.\n",
      "\n",
      "Artificial Intelligence, a vast domain,\n",
      "Includes technologies, both known and unknown.\n",
      "Machine Learning and Deep Learning, two of its children,\n",
      "Are part of this world, where they've been sent.\n",
      "\n",
      "So, let's embrace this change, this digital tide,\n",
      "And let Artificial Intelligence be our guide.\n",
      "For in its hands, the future resides,\n",
      "A world transformed, with new strides.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEndpoint  # Or the specific LLM library you're using\n",
    "\n",
    "# Example setup for LLM (ensure this is compatible with your LLM)\n",
    "def setup_llm():\n",
    "    return HuggingFaceEndpoint(\n",
    "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",  # Replace with the appropriate model\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "# Sample documents\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning automates data analysis.\"),\n",
    "    Document(page_content=\"Deep learning uses neural networks to learn patterns.\"),\n",
    "    Document(page_content=\"Artificial intelligence includes various technologies.\"),\n",
    "]\n",
    "\n",
    "# Test the format_docs function\n",
    "formatted_docs = format_docs(sample_docs)\n",
    "print(\"Formatted Documents:\\n\")\n",
    "print(formatted_docs)\n",
    "\n",
    "# Test the style_prompt with sample inputs\n",
    "style = \"poetic\"\n",
    "context = formatted_docs\n",
    "original_text = \"Artificial intelligence is transforming the world.\"\n",
    "\n",
    "styled_prompt = style_prompt.format(\n",
    "    style=style,\n",
    "    context=context,\n",
    "    original_text=original_text,\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Prompt for Style Transfer:\\n\")\n",
    "print(styled_prompt)\n",
    "\n",
    "# Pass the prompt to the LLM\n",
    "llm = setup_llm()  # Initialize the LLM\n",
    "styled_output = llm(styled_prompt)  # Generate the styled text\n",
    "\n",
    "print(\"\\n--- Rewritten (Styled) Text ---\")\n",
    "print(styled_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq3HaHd0CFog"
   },
   "source": [
    "# 11. RAG chain\n",
    "\n",
    "In this task, students will implement a RAG chain that integrates an ensemble retriever (Chroma and BM25), formats retrieved context, applies a prompt template, and generates styled output using a Language Model (LLM).\n",
    "\n",
    "The goal is to:\n",
    "\n",
    "Use the EnsembleRetriever to retrieve relevant documents from Chroma and BM25.\n",
    "Format the retrieved documents into a readable context.\n",
    "Generate a prompt for neural style transfer using the retrieved context and the input query.\n",
    "Pass the prompt to the LLM and parse the model's response to return the final styled output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbvH2EZ9zp-d"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def build_rag_chain(llm, chroma_store, bm25_retriever):\n",
    "    \"\"\"\n",
    "    Build a RAG chain using an ensemble retriever with Chroma and BM25,\n",
    "    followed by formatting the context, applying the prompt, and parsing the output.\n",
    "\n",
    "    Parameters:\n",
    "    - llm: The language model for generating styled text.\n",
    "    - chroma_store: Chroma vector store for semantic retrieval.\n",
    "    - bm25_retriever: BM25 retriever for lexical retrieval.\n",
    "\n",
    "    Returns:\n",
    "    - rag_chain: A function that processes inputs through the RAG pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Define the Ensemble Retriever\n",
    "    ensemble_retriever = EnsembleRetriever(chroma_store,bm25_retriever)  # Replace with your implementation.\n",
    "\n",
    "    # Step 2: Define a function to retrieve and format context\n",
    "    def retrieve_and_format_context(query, k=5):\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents and format them into a readable context.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The input query.\n",
    "        - k (int): The number of documents to retrieve (default: 5).\n",
    "\n",
    "        Returns:\n",
    "        - str: The formatted context string.\n",
    "        \"\"\"\n",
    "        # Step 2.1: Retrieve relevant documents using the ensemble retriever.\n",
    "        context_docs = ensemble_retriever.get_relevant_documents(query,k=k)  # Replace with your implementation.\n",
    "\n",
    "        # Step 2.2: Format the retrieved documents.\n",
    "        context = format_docs(context_docs)  # Replace with your implementation.\n",
    "\n",
    "        return context\n",
    "\n",
    "    # Step 3: Define the RAG chain\n",
    "    def rag_chain(inputs):\n",
    "        \"\"\"\n",
    "        Process inputs through the RAG pipeline to generate styled output.\n",
    "\n",
    "        Parameters:\n",
    "        - inputs (dict): A dictionary containing:\n",
    "            - \"question\" (str): The query for retrieving context.\n",
    "            - \"style\" (str): The desired writing style.\n",
    "            - \"original_text\" (str): The text to be rewritten.\n",
    "\n",
    "        Returns:\n",
    "        - str: The final styled output.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 3.1: Retrieve and format the context using the helper function.\n",
    "        query = inputs[\"question\"]\n",
    "        context = retrieve_and_format_context(query,k=5)  # Replace with your implementation.\n",
    "\n",
    "        # Step 3.2: Generate the prompt using the `style_prompt`.\n",
    "        prompt = style_prompt.format(\n",
    "            style=inputs[\"style\"],\n",
    "            context=context,\n",
    "            original_text=inputs[\"original_text\"]\n",
    "        )  # Replace with your implementation.\n",
    "\n",
    "        # Step 3.3: Pass the prompt through the LLM to generate the output.\n",
    "        llm_output = llm(prompt)  # Replace with your implementation.\n",
    "\n",
    "        # Step 3.4: Parse the LLM's output to extract the final styled text.\n",
    "        parse_passthrough = RunnablePassthrough()\n",
    "        parser = parse_passthrough.invoke(llm_output)  # Replace with your implementation.\n",
    "        result = parser  # Replace with your implementation.\n",
    "\n",
    "        return result\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0NBNIFCDgvE"
   },
   "source": [
    "# 12. Final response\n",
    "\n",
    "In this task, students will implement the main script that integrates all components of the RAG pipeline into a complete application. The script will:\n",
    "\n",
    "1. Scrape content from specified URLs, process the raw text, and split it into smaller, retrievable chunks.\n",
    "2. Build the retrievers: Create a Chroma vector store and a BM25 retriever using the processed documents.\n",
    "3. Build the RAG chain: Set up a pipeline that integrates the retrievers, context formatting, and an LLM to perform neural style transfer.\n",
    "4. Run the application: Accept a user query and a target style, then process the input through the RAG chain to produce styled output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "I1pFwYzOCZvj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Scraping content and splitting into documents...\n",
      "Scraping content from: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "Scraping content from: https://en.wikipedia.org/wiki/Machine_learning\n",
      "Total number of documents: 373\n",
      "Step 2: Building Chroma vector store and BM25 retriever...\n",
      "Step 3: Building RAG chain...\n",
      "\n",
      "Step 4: Neural Style Transfer Demo...\n",
      "\n",
      "============================================\n",
      "        Neural Style Transfer Demo          \n",
      "============================================\n",
      "Original Text : Explain machine learning.\n",
      "Desired Style : as if it were a recipe for cooking\n",
      "\n",
      "Step 5: Running the RAG chain...\n",
      "\n",
      "--- Styled Output ---\n",
      "\n",
      "\n",
      "Preparing Your Ingredients:\n",
      "1. Gather a collection of data (your ingredients)\n",
      "2. Clean the data (wash and chop the ingredients)\n",
      "3. Divide the data into training and testing sets (separate the ingredients into two bowls)\n",
      "\n",
      "Selecting Your Recipe:\n",
      "1. Choose the type of machine learning that best suits your needs:\n",
      "   - Unsupervised learning (analyze the data and find patterns without any guidance)\n",
      "   - Supervised learning (label the training data with the expected answers and learn to make predictions based on that)\n",
      "     - Classification (learn to predict what category the input belongs to)\n",
      "     - Regression (deduce a numeric function based on the numeric input)\n",
      "   - Reinforcement learning (receive rewards for good responses and punishments for bad ones)\n",
      "   - Transfer learning (apply the knowledge gained from one problem to a new problem)\n",
      "\n",
      "Cooking Your Machine Learning Dish:\n",
      "1. Prepare your machine learning model (choose the appropriate ingredients for your dish)\n",
      "2. Train your model on the training data (cook the ingredients together)\n",
      "3. Test your model on the testing data (taste the dish to see if it's to your liking)\n",
      "4. Adjust the model as needed (add more ingredients or adjust cooking time to improve the taste)\n",
      "\n",
      "Serving Your Machine Learning Dish:\n",
      "1. Use your trained model to make predictions on new data (serve the dish to guests)\n",
      "2. Monitor the model's performance and adjust as necessary (ensure the dish continues to please your guests)\n",
      "\n",
      "Enjoy your machine learning dish! And remember, just like cooking, the art of machine learning requires patience, practice, and continuous improvement.\n",
      "\n",
      "[Recipe adapted from Machine Learning - Wikipedia]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main script for scraping, building retrievers, setting up the RAG chain,\n",
    "    and running a neural style transfer demo.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Scrape content and split into documents\n",
    "    print(\"Step 1: Scraping content and splitting into documents...\")\n",
    "    example_urls = [\n",
    "        \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "        \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "    ]\n",
    "\n",
    "    # Step 1A: Initialize an empty list to store all documents\n",
    "    all_docs = []\n",
    "\n",
    "    # Step 1B: Iterate through the URLs to fetch and process content\n",
    "    for url in example_urls:\n",
    "        print(f\"Scraping content from: {url}\")\n",
    "\n",
    "        # Step 1B.1: Fetch and parse the raw text from the URL\n",
    "        raw_text = fetch_and_parse(url)\n",
    "\n",
    "        # Step 1B.2: Split the raw text into chunks (documents)\n",
    "        splits = split_text_into_documents(raw_text)\n",
    "\n",
    "        # Step 1B.3: Add the chunks to the list of documents\n",
    "        all_docs.extend(splits)\n",
    "\n",
    "    print(f\"Total number of documents: {len(all_docs)}\")\n",
    "\n",
    "    # Step 2: Build Chroma and BM25 retrievers\n",
    "    print(\"Step 2: Building Chroma vector store and BM25 retriever...\")\n",
    "\n",
    "    # Step 2A: Build the Chroma vector store\n",
    "    chroma_store = build_chroma(all_docs)  # Replace with your implementation\n",
    "\n",
    "    # Step 2B: Build the BM25 retriever\n",
    "    bm25_retriever = BM25Retriever(all_docs)  # Replace with your implementation\n",
    "\n",
    "    # Step 3: Build the RAG chain\n",
    "    print(\"Step 3: Building RAG chain...\")\n",
    "\n",
    "    # Step 3A: Set up the LLM\n",
    "    llm = setup_llm()  # Replace with your implementation\n",
    "\n",
    "    # Step 3B: Build the RAG chain\n",
    "    rag_chain = build_rag_chain(llm,chroma_store,bm25_retriever)  # Replace with your implementation\n",
    "\n",
    "    # Step 4: Neural Style Transfer Demo\n",
    "    print(\"\\nStep 4: Neural Style Transfer Demo...\")\n",
    "\n",
    "    # Step 4A: Define the user query and target style\n",
    "    user_text = \"Explain machine learning.\"\n",
    "    target_style = \"as if it were a recipe for cooking\"\n",
    "    inputs = {\"question\": user_text, \"style\": target_style, \"original_text\": user_text}\n",
    "\n",
    "    print(\"\\n============================================\")\n",
    "    print(\"        Neural Style Transfer Demo          \")\n",
    "    print(\"============================================\")\n",
    "    print(f\"Original Text : {user_text}\")\n",
    "    print(f\"Desired Style : {target_style}\")\n",
    "\n",
    "    # Step 5: Run the RAG chain\n",
    "    print(\"\\nStep 5: Running the RAG chain...\")\n",
    "\n",
    "    # Hint: Pass `inputs` through the RAG chain to generate styled output.\n",
    "    styled_result = rag_chain(inputs)  # Replace with your implementation\n",
    "\n",
    "    print(\"\\n--- Styled Output ---\")\n",
    "    print(styled_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVYsup1kFTUW"
   },
   "source": [
    "***What You Should Remember:***\n",
    "\n",
    "1. RAG (Retrieval-Augmented Generation) combines the power of information\n",
    "retrieval and language models to generate accurate and context-aware responses.\n",
    "\n",
    "2. Chroma Vector Store is used for semantic retrieval by embedding documents into high-dimensional vectors and finding semantically similar documents for a given query.\n",
    "\n",
    "3. BM25 Retriever uses lexical matching to rank documents based on the occurrence of query terms, ensuring precision in keyword-based searches.\n",
    "\n",
    "4. Ensemble Retriever merges results from Chroma (semantic similarity) and BM25 (lexical matching) to provide a balance of relevance and diversity in retrieved documents.\n",
    "\n",
    "5. Formatting Context ensures that retrieved documents are clean, readable, and useful for the LLM, improving the quality of generated outputs.\n",
    "\n",
    "6. Prompt Templates guide the LLM by structuring inputs, specifying the task (e.g., style transfer), and ensuring clarity and relevance.\n",
    "\n",
    "7. Neural Style Transfer enables the LLM to rewrite text in a specified style (e.g., formal, poetic, conversational) using both the original input and retrieved context.\n",
    "\n",
    "8. Building a RAG pipeline requires:\n",
    "\n",
    "    **Data preparation:** Scraping and splitting raw text into smaller, retrievable chunks.\n",
    "\n",
    "    **Retriever setup:** Combining Chroma and BM25 to maximize retrieval quality.\n",
    "\n",
    "    **Chain integration:** Connecting the retrievers, context formatting, and LLM to form a cohesive workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQ1Eo7i5GSDT"
   },
   "source": [
    "Congratulations! You've come to the end of this assignment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
